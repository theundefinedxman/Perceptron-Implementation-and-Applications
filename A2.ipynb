{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31432457-eaf4-48e5-a471-8e84315926c1",
   "metadata": {},
   "source": [
    "# CSC2042S Machine Learning @ UCT\n",
    "# Supervised Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f9d85-7e0d-4933-991f-44ecff783c1e",
   "metadata": {},
   "source": [
    "**Author: Buqwana Xolisile**\n",
    "\n",
    "This notebook implements a binary classifier extended to multi-class classification using the **one-vs-rest** approach. It begins by loading the `simpsons-mnist-master` dataset using the **Loader** class and performing data preprocessing, including normalization and splitting into training, validation, and test sets.\n",
    "\n",
    "Next, the **BinaryPerceptron** and **MultiClassPerceptron** classes are initialized. The perceptron models are trained on both grayscale and RGB images, with the **PerceptronTrainer** class handling weight updates, data shuffling, and different stopping criteria such as fixed epochs, error threshold, and early stopping.\n",
    "\n",
    "After training, the models are evaluated on the validation and test sets. Key metrics such as accuracy, precision, recall, and F1 score are computed to assess classification performance. Confusion matrices are also visualized to identify which characters are well-classified and where misclassifications occur.\n",
    "\n",
    "Finally, hyperparameter tuning is performed using the **HyperparameterTuner** class, exploring different learning rates, weight initialization methods, and normalization techniques. The best-performing models are selected based on validation accuracy, and their performance is compared between grayscale and RGB inputs, highlighting the effect of color information on multi-class character recognition.\n",
    "\n",
    "\n",
    "**Outline:**\n",
    "\n",
    "* [1. Data processing](#1.-data-processing)\n",
    "* [2. Multi-class perceptron implementation](#section2)\n",
    "* [3. Training](#section3)\n",
    "* [4. Hyperparameter tuning](#section4)\n",
    "* [5. Evaluation](#section5)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e36739-9a4b-4ef3-ab12-48aab91cd7cd",
   "metadata": {},
   "source": [
    "## Imports, Installations, and Downloads\n",
    "\n",
    "This notebook will make extensive use of some standard Python libraries for scientific computing, namely:\n",
    "* ``numpy`` for storing and manipulating arrays/matrices of numerical data.\n",
    "* ``matplotlib`` for displaying image data.\n",
    "* ``PIL (Image)`` for opening and processing images.\n",
    "* ``random`` for generating random numbers and shuffling data.\n",
    "* ``os`` for interacting with files and directories.\n",
    "* ``struct`` for handling binary data formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c23d5-b10e-4097-9bd4-2e6e03443206",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import struct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef87bd82-57d5-41a0-a566-b74413fcf4dd",
   "metadata": {},
   "source": [
    "# 1. Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b819372-dd48-45f0-b39e-e7267c91a19f",
   "metadata": {},
   "source": [
    "We will now load the `Simpsons-MNIST` which is a small dataset of The Simpsons characters consisting of a training set of 8,000 examples\n",
    "and a test set of 2,000 examples. Each example is a 28x28 RGB/grayscale image, associated with a label from 10 classes.\n",
    "This dataset is available in both formats, RGB and grayscale available at: [MNIST](https://github.com/google/n-digit-mnist)\n",
    "We use the defined class below **Loader** which has the following functions which works as follows:\n",
    "\n",
    "The **load_images_from_directory** function takes a folder containing subfolders for each character, and each subfolder is assigned a numeric label according to the label_map. It then goes through all the JPEG images in each subfolder, loads them using PIL, and converts them into NumPy arrays. For every image, the corresponding label of its folder is also stored. Finally, the function returns two NumPy arrays: one containing all the images and one containing their associated labels.Also the **normalize_images** function which converts image pixel values to floating-point numbers and scales them to a range between 0 and 1.\n",
    "\n",
    "**flatten_images** this function flattens each image into a 1D vector by reshaping the array from (n_samples, height, width) or (n_samples, height, width, channels) to (n_samples, features) for perceptron modeling,then the **split_train_validation function** takes the training data and splits it into training and validation sets. It randomly shuffles the data, selects a 20% portion for validation, and keeps the rest for training then returns both the new training set and the validation set.\n",
    "\n",
    "Finally **load_rgb_data** and **load_grayscale_data** functions that will mainly act as wrappers that call earlier helper functions that handle the process of loading images, splitting them into training and validation sets, normalizing pixel values, and flattening them into vectors in which they return clean datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d6cbf-2808-4f5d-8a6f-97d085cb3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    \"\"\"\n",
    "    Simpsons-MNIST JPEG Image Loader\n",
    "    Loads images from 'simpsons-mnist-master/dataset/grayscale' and 'simpsons-mnist-master/dataset/rgb'\n",
    "    directories containing 'test' and 'train' folders with JPEG files.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir='simpsons-mnist-master/dataset'):\n",
    "        self.base_dir = base_dir\n",
    "        self.grayscale_dir = os.path.join(base_dir, 'grayscale')\n",
    "        self.rgb_dir = os.path.join(base_dir, 'rgb')\n",
    "        \n",
    "    def load_images_from_directory(self, directory_path):\n",
    "        \"\"\"Load all JPEG images from a directory and return as numpy array\"\"\"\n",
    "        images = []\n",
    "        labels = []\n",
    "        label_map = {\n",
    "            \"bart_simpson\": 0,\n",
    "            \"charles_montgomery_burns\": 1,\n",
    "            \"homer_simpson\": 2,\n",
    "            \"krusty_the_clown\": 3,\n",
    "            \"lisa_simpson\": 4,\n",
    "            \"marge_simpson\": 5,\n",
    "            \"milhouse_van_houten\": 6,\n",
    "            \"moe_szyslak\": 7,\n",
    "            \"ned_flanders\": 8, \n",
    "            \"principal_skinner\": 9\n",
    "        }\n",
    "        \n",
    "        character_folders = [f for f in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, f))]  # Get all character folders \n",
    "        for folder_name in character_folders:\n",
    "            label = label_map[folder_name]\n",
    "            \n",
    "            folder_path = os.path.join(directory_path, folder_name)\n",
    "            image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg'))]  # Get all JPEG files in this character folder\n",
    "    \n",
    "            for filename in image_files:\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with Image.open(file_path) as img: # Load image using PIL\n",
    "                    img_array = np.array(img) # Convert to numpy array\n",
    "                    images.append(img_array)\n",
    "                    labels.append(label)\n",
    "        return np.array(images), np.array(labels)\n",
    "        \n",
    "    def normalize_images(self, images):\n",
    "        \"\"\"Normalize images to [0,1] range\"\"\"\n",
    "        return images.astype(np.float32) / 255.0\n",
    "        \n",
    "    def flatten_images(self, images):\n",
    "        \"\"\"Flatten images into vectors for perceptron modeling.\"\"\"\n",
    "        n_samples = images.shape[0]\n",
    "        return images.reshape(n_samples, -1)\n",
    "\n",
    "    def split_train_validation(self, X_train, y_train, validation_ratio=0.25):\n",
    "        \"\"\"Split training data into training and validation sets\"\"\"\n",
    "        n_samples = len(X_train)\n",
    "        n_validation = int(n_samples * validation_ratio)\n",
    "    \n",
    "        indices = np.random.permutation(n_samples)  # Create random indices for splitting\n",
    "        val_indices = indices[:n_validation]\n",
    "        train_indices = indices[n_validation:]\n",
    "        \n",
    "        X_valid = X_train[val_indices]\n",
    "        y_valid = y_train[val_indices]\n",
    "        X_train_split = X_train[train_indices]\n",
    "        y_train_split = y_train[train_indices]\n",
    "        \n",
    "        return (X_train_split, y_train_split), (X_valid, y_valid)\n",
    "\n",
    "    def load_grayscale_data(self):\n",
    "        \"\"\"Load and process grayscale MNIST data\"\"\"\n",
    "        train_dir = os.path.join(self.grayscale_dir, 'train')\n",
    "        test_dir = os.path.join(self.grayscale_dir, 'test')\n",
    "        \n",
    "        X_train_raw, y_train = self.load_images_from_directory(train_dir) # Load training \n",
    "        X_test_raw, y_test = self.load_images_from_directory(test_dir) # Load Test data\n",
    "        \n",
    "        (X_train_raw, y_train), (X_val_raw, y_valid) = self.split_train_validation(X_train_raw, y_train)  # Split training data into train and validation\n",
    "\n",
    "        X_train_raw = self.normalize_images(X_train_raw)\n",
    "        X_val_raw = self.normalize_images(X_val_raw)\n",
    "        X_test_raw = self.normalize_images(X_test_raw)\n",
    "        \n",
    "        X_train = self.flatten_images(X_train_raw)\n",
    "        X_valid = self.flatten_images(X_val_raw)\n",
    "        X_test = self.flatten_images(X_test_raw)\n",
    "        \n",
    "        return (X_train, y_train), ( X_valid, y_valid), (X_test, y_test)\n",
    "    \n",
    "    def load_rgb_data(self):\n",
    "        \"\"\"Load and process RGB MNIST data\"\"\"\n",
    "        train_dir = os.path.join(self.rgb_dir, 'train')\n",
    "        test_dir = os.path.join(self.rgb_dir, 'test')\n",
    "        \n",
    "        X_train_raw, y_train = self.load_images_from_directory(train_dir)  # Load training\n",
    "        X_test_raw, y_test = self.load_images_from_directory(test_dir)  # Test data\n",
    "        \n",
    "        (X_train_raw, y_train), (X_val_raw, y_valid) = self.split_train_validation(X_train_raw, y_train)  # Split training data into train and validation\n",
    "\n",
    "        X_train_raw = self.normalize_images(X_train_raw)\n",
    "        X_val_raw = self.normalize_images(X_val_raw)\n",
    "        X_test_raw = self.normalize_images(X_test_raw)\n",
    "        \n",
    "        X_train = self.flatten_images(X_train_raw)\n",
    "        X_valid = self.flatten_images(X_val_raw)\n",
    "        X_test = self.flatten_images(X_test_raw)\n",
    "        \n",
    "        return (X_train, y_train), (X_valid, y_valid), (X_test, y_test)\n",
    "    \n",
    "    def load_all_data(self):\n",
    "        \"\"\"Load both grayscale and RGB data\"\"\"\n",
    "        grayscale_data = self.load_grayscale_data()\n",
    "        rgb_data = self.load_rgb_data() \n",
    "        return {\n",
    "            'grayscale': {\n",
    "                'train': grayscale_data[0],\n",
    "                'validation': grayscale_data[1],\n",
    "                'test': grayscale_data[2]\n",
    "            },\n",
    "            'rgb': {\n",
    "                'train': rgb_data[0],\n",
    "                'validation': rgb_data[1],\n",
    "                'test': rgb_data[2]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def visualize_sample(self, images, labels, n_samples=10, title=\"Sample Images\"):\n",
    "        \"\"\"Visualize sample images\"\"\"\n",
    "        fig, axes = plt.subplots(1, n_samples, figsize=(12, 3))\n",
    "        for i in range(n_samples):\n",
    "            if images.shape[1] == 784:  # Grayscale\n",
    "                    img = images[i].reshape(28, 28)\n",
    "                    axes[i].imshow(img, cmap='gray')\n",
    "            elif images.shape[1] == 2352:  # RGB (28x28x3)\n",
    "                    img = images[i].reshape(28, 28, 3)\n",
    "                    axes[i].imshow(img)\n",
    "            axes[i].set_title(f'Label: {labels[i]}')\n",
    "            axes[i].axis('off')\n",
    "            \n",
    "        plt.suptitle(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc2fc88-261f-440f-b12a-129bd35bbf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    loader = Loader('simpsons-mnist-master/dataset') # Initialize the loader class.\n",
    "    try:\n",
    "        print(\"Data preprocessing in progress......\") \n",
    "        all_data = loader.load_all_data()\n",
    "        grayscale_train = all_data['grayscale']['train']\n",
    "        grayscale_val = all_data['grayscale']['validation']\n",
    "        grayscale_test = all_data['grayscale']['test']\n",
    "        \n",
    "        rgb_train = all_data['rgb']['train']\n",
    "        rgb_val = all_data['rgb']['validation']\n",
    "        rgb_test = all_data['rgb']['test']\n",
    "        \n",
    "        print(f\"Grayscale - Training: {grayscale_train[0].shape}, Validation: {grayscale_val[0].shape}, Test: {grayscale_test[0].shape}\")\n",
    "        print(f\"RGB -  Training:  {rgb_train[0].shape}, Validation: {rgb_val[0].shape}, Test: {rgb_test[0].shape}\")\n",
    "        print(\"Data loading completed successfully!\")\n",
    "        loader.visualize_sample(grayscale_train[0][:10], grayscale_train[1][:10], title=\"Grayscale Training Samples\")  \n",
    "        loader.visualize_sample(grayscale_val[0][:10], grayscale_val[1][:10], title=\"Grayscale Validation Samples\")  \n",
    "        loader.visualize_sample(grayscale_test[0][:10], grayscale_test[1][:10], title=\"Grayscale Test Samples\")  \n",
    "       \n",
    "        loader.visualize_sample(rgb_train[0][:10], rgb_train[1][:10], title=\"RGB Training Samples\")  \n",
    "        loader.visualize_sample(rgb_val[0][:10], rgb_val[1][:10], title=\"RGB Validation Samples\")   \n",
    "        loader.visualize_sample(rgb_test[0][:10], rgb_test[1][:10], title=\"RGB Test Samples\")    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Could not find directory. Please ensure the directory structure exists:\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb25bc1-b89e-4b61-83ca-347aff6eec18",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "# 2. Multi-class perceptron implementation \n",
    "# Binary Perceptron\n",
    "We will now define the `BinaryPerceptron` which is a simple classifier that predicts binary labels (0 or 1) using a linear decision boundary.  \n",
    "The perceptron is initialized with the number of input features and a learning rate `alpha`; it assigns small random weights (sampled from a Gaussian) to each feature and sets the bias to zero so the model starts with a random decision boundary that is later adjusted during training.\n",
    "\n",
    "The **predict** function computes the weighted sum of an input vector plus the bias and returns `1` when that sum is greater than or equal to zero and `0` otherwise, implementing the step activation rule used for binary decisions.  The **weighted_sum** function returns the raw linear combination `w·x + b` without applying the threshold, which is useful for inspection and debugging.\n",
    "\n",
    "The **apply_learning_rule** function is the core of learning: it first computes the current prediction, compares it with the true label, and updates each weight and the bias according to the perceptron learning rule `w <- w + alpha * (y - y_hat) * x` and `b <- b + alpha * (y - y_hat)`, thereby nudging the decision boundary to reduce classification errors over time.\n",
    "\n",
    "Finally, the **__repr__** function returns a readable string describing the perceptron’s current weights, bias, and learning rate so the model state can be inspected easily during experiments.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8effbe28-7b73-4461-ac54-35b189ac7229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class BinaryPerceptron:\n",
    "    \"\"\"\n",
    "    Binary Perceptron classifier implementing the perceptron learning rule.\n",
    "    Predicts binary labels (0 or 1) using a linear decision boundary.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_of_features, alpha=0.01):\n",
    "        \"\"\"Initialize the Binary Perceptron.\"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.weights = np.random.normal(0, 0.1, num_of_features)\n",
    "        self.bias = 0.0\n",
    "        \n",
    "    def predict(self, x): \n",
    "        \"\"\"Predict 0 or 1 depending on whether weighted sum >= 0.\"\"\"  \n",
    "        return np.where(np.dot(x, self.weights) + self.bias >= 0, 1, 0)\n",
    "        \n",
    "    def weighted_sum(self, x):\n",
    "        \"\"\"Returns weighted sum.\"\"\"\n",
    "        return np.dot(x, self.weights) + self.bias\n",
    "        \n",
    "    def apply_learning_rule(self, x, y):\n",
    "        \"\"\"\n",
    "        Update weights and bias using the perceptron learning rule:\n",
    "          w_i <- w_i + α (y - g(x)) x_i\n",
    "          b   <- b   + α (y - g(x))\n",
    "        \"\"\"\n",
    "        y_hat = self.predict(x)\n",
    "        error = y - y_hat\n",
    "        self.weights = self.weights + self.alpha * error * x\n",
    "        self.bias = self.bias + self.alpha * error\n",
    " \n",
    "    def __repr__(self):\n",
    "        return f\"BinaryPerceptron(weights={self.weights}, bias={self.bias:.3f}, alpha={self.alpha})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cef66fa-294e-4426-a3f3-5f8b2a576572",
   "metadata": {},
   "source": [
    "<a id='section2.1'></a>\n",
    "# Multi-class perceptron\n",
    "While a  `BinaryPerceptron` can only decide between two outcomes, the multi-class version expands this idea to ten possible labels. It does this by keeping a small team of perceptrons, one dedicated to each class. This is known as the One-vs-Rest strategy. \n",
    "\n",
    "We will now define the `MultiClassPerceptron`, which extends the `BinaryPerceptron` to perform multi-class classification using a **One-vs-Rest (OvR)** strategy as mentioned. The model maintains one `BinaryPerceptron` per class, so for a dataset with 10 classes, there are 10 perceptrons, each trained to distinguish its class from all others.\n",
    "\n",
    "The **__init__** method initializes these perceptrons, using a unique random seed for each one to ensure diverse initial weights and independent decision boundaries.  The **predict** method computes the weighted sum for each perceptron and selects the class with the highest score.  \n",
    "This works for both single inputs and batches, making the model flexible for inference on multiple samples at once.\n",
    "\n",
    "The **apply_learning_rule** method updates all perceptrons in a One-vs-Rest fashion: the perceptron for the correct class is trained with a positive label (`1`), while all others are trained with a negative label (`0`). This process reinforces the correct class while pushing down the scores of the incorrect ones, improving classification performance over time.\n",
    "\n",
    "Finally, the **__repr__** method provides a readable summary showing the number of classes and the learning rate, making it easy to inspect the model’s configuration and training setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b50ae5-29a6-4e2e-8465-cc6c57351efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassPerceptron:\n",
    "    \"\"\"\n",
    "    Multi-class Perceptron using One-vs-Rest strategy.\n",
    "    Maintains one BinaryPerceptron per class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_of_features, alpha=0.01):\n",
    "        self.num_classes = 10\n",
    "        self.perceptrons = []\n",
    "        \n",
    "        for i in range(self.num_classes):\n",
    "            np.random.seed(42 + i)  # Different seed for each perceptron\n",
    "            perceptron = BinaryPerceptron(num_of_features, alpha)\n",
    "            self.perceptrons.append(perceptron)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for input X.\n",
    "        X can be a single sample (1D) or batch of samples (2D).\n",
    "        Returns predictions for each sample.\n",
    "        \"\"\"\n",
    "        if X.ndim == 1:\n",
    "            scores = [p.weighted_sum(X) for p in self.perceptrons]  # Single sample\n",
    "            return np.argmax(scores)\n",
    "        else:\n",
    "            predictions = []  # Batch of samples\n",
    "            for sample in X:\n",
    "                scores = [p.weighted_sum(sample) for p in self.perceptrons]\n",
    "                predictions.append(np.argmax(scores))\n",
    "            return np.array(predictions)\n",
    "    \n",
    "    def apply_learning_rule(self, x, y):\n",
    "        \"\"\"\n",
    "        Train all perceptrons in One-vs-Rest fashion.\n",
    "        x is a single sample, y is the true class label (0..num_classes-1).\n",
    "        \"\"\"\n",
    "        for i, perceptron in enumerate(self.perceptrons):\n",
    "            binary_label = 1 if i == y else 0\n",
    "            perceptron.apply_learning_rule(x, binary_label)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"MultiClassPerceptron(num_classes={self.num_classes}, alpha={self.perceptrons[0].alpha})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc10ddcd-9fa1-4d65-aebb-84696dc7cb04",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "# 3. Training\n",
    "Now we want to train our models to see how they model and respond to input data given to them. To do this, we will now define the `PerceptronTrainer`, a class designed to handle the training of a `BinaryPerceptron` with flexible stopping criteria. This trainer accepts training data and optional validation data and supports three types of stopping: a fixed number of epochs, an error threshold, and early stopping based on validation accuracy.\n",
    "\n",
    "The **__init__** method sets up the perceptron, datasets, stopping parameters, and internal trackers for epochs and validation performance. The **shuffle_data** method randomly shuffles the training data at the start of each epoch so the perceptron sees the samples in a different order every time, which helps improve generalization and reduces bias from sample order. The **accuracy** method computes how many predictions are correct as a fraction of total samples, allowing us to measure model performance on both training and validation sets.\n",
    "\n",
    "The **stopping_criteria** method checks whether training should stop. For fixed epochs, training stops when the maximum number of epochs is reached. For error threshold stopping, it halts when the training error drops below a chosen value. For early stopping, it monitors validation accuracy and stops if no improvement occurs over a set number of epochs (patience), helping prevent overfitting.\n",
    "\n",
    "The **train** method runs the main training loop: for each epoch it shuffles the data, applies the perceptron learning rule to each sample, computes training accuracy, prints progress, and checks whether the stopping condition is met. When training completes, it returns the trained perceptron.This trainer provides a consistent and convenient way to experiment with different training strategies and observe how the perceptron learns from data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec68214-f901-450e-9ff8-55f2cc5ce52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronTrainer:\n",
    "    \"\"\"Trainer class for Binary Perceptron.\"\"\"\n",
    "    def __init__(self, perceptron, X_train, y_train, X_val=None, y_val=None,\n",
    "                 stopping_criterion=\"fixed_epochs\", total_epochs=50,\n",
    "                 error_threshold=0.05, patience=5, min_delta=1e-4):\n",
    "        \"\"\"Initialize trainer with options for stopping criteria.\"\"\"\n",
    "        self.perceptron = perceptron\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "        self.stopping_criterion = stopping_criterion\n",
    "        self.total_epochs = total_epochs  \n",
    "        self.error_threshold = error_threshold\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "        self.epochs_trained = 0\n",
    "        \n",
    "        self.stopped_reason = None\n",
    "        self.epochs_without_improvement = 0\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def shuffle_data(self, X, y):\n",
    "        \"\"\"Return shuffled copies of X and y.\"\"\"\n",
    "        indices = np.random.permutation(len(X))\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"Compute accuracy for given data.\"\"\"\n",
    "        return np.mean(self.perceptron.predict(X) == y)\n",
    "        \n",
    "    def stopping_criteria(self, epoch, train_error_rate, val_accuracy=None):\n",
    "        \"\"\"Checks stopping criteria and returns True if training should stop.\"\"\"\n",
    "        if self.stopping_criterion == 'fixed_epochs':\n",
    "            if epoch >= self.total_epochs:   \n",
    "                self.stopped_reason = f\"Reached total epochs ({self.total_epochs})\"\n",
    "                return True\n",
    "    \n",
    "        elif self.stopping_criterion == 'error_threshold':\n",
    "            if train_error_rate <= self.error_threshold:\n",
    "                self.stopped_reason = (\n",
    "                    f\"Training error ({train_error_rate:.4f}) \"\n",
    "                    f\"below threshold ({self.error_threshold})\"\n",
    "                )\n",
    "                return True\n",
    "            if epoch >= self.total_epochs:  \n",
    "                self.stopped_reason = (\n",
    "                    f\"Reached total epochs ({self.total_epochs}) \"\n",
    "                    f\"without meeting error threshold\"\n",
    "                )\n",
    "                return True\n",
    "    \n",
    "        elif self.stopping_criterion == 'early_stopping':\n",
    "            if val_accuracy is None:\n",
    "                raise ValueError(\"Validation data required for early stopping\")\n",
    "                \n",
    "            best_val_acc = max(self.val_accuracies) if self.val_accuracies else 0.0\n",
    "    \n",
    "            if val_accuracy <= best_val_acc + self.min_delta:\n",
    "                self.epochs_without_improvement += 1\n",
    "            else:\n",
    "                self.epochs_without_improvement = 0\n",
    "    \n",
    "            if self.epochs_without_improvement >= self.patience:\n",
    "                self.stopped_reason = (\n",
    "                    f\"No improvement for {self.patience} epochs (early stopping)\"\n",
    "                )\n",
    "                return True\n",
    "    \n",
    "            if epoch >= self.total_epochs:\n",
    "                self.stopped_reason = f\"Reached total epochs ({self.total_epochs})\"\n",
    "                return True\n",
    "    \n",
    "        return False\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train perceptron according to the chosen stopping criterion.\"\"\"      \n",
    "        for epoch in range(1, self.total_epochs + 1):\n",
    "            X_shuffled, y_shuffled = self.shuffle_data(self.X_train, self.y_train)\n",
    "            for x, y in zip(X_shuffled, y_shuffled):\n",
    "                self.perceptron.apply_learning_rule(x, y)\n",
    "                    \n",
    "            train_acc = self.accuracy(self.X_train, self.y_train)\n",
    "            train_error_rate = 1 - train_acc\n",
    "    \n",
    "            val_accuracy = None\n",
    "            if self.X_val is not None and self.y_val is not None:\n",
    "                val_accuracy = self.accuracy(self.X_val, self.y_val)\n",
    "                self.val_accuracies.append(val_accuracy)\n",
    "                print(f\"Epoch {epoch}/{self.total_epochs} | Train Acc: {train_acc*100:.2f}% | Val Acc: {val_accuracy*100:.2f}%\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch}/{self.total_epochs} | Train Acc: {train_acc*100:.2f}%\")\n",
    "\n",
    "            self.epochs_trained = epoch\n",
    "            \n",
    "            if self.stopping_criteria(epoch, train_error_rate, val_accuracy):\n",
    "                print(f\"Stopping: {self.stopped_reason}\")\n",
    "                break\n",
    "    \n",
    "        print(\"Training finished.\")\n",
    "        return self.perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775cfe26-5774-4ba0-a203-bc29af8c0eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    loader = Loader('simpsons-mnist-master/dataset')\n",
    "    try:\n",
    "        print(\"Data preprocessing in progress......\") \n",
    "        all_data = loader.load_all_data()\n",
    "        \n",
    "        X_train, y_train = all_data['grayscale']['train']\n",
    "        X_val, y_val = all_data['grayscale']['validation']\n",
    "        X_test, y_test = all_data['grayscale']['test']\n",
    "        print(f\"Grayscale - Training: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        X_train_rgb, y_train_rgb = all_data['rgb']['train']\n",
    "        X_val_rgb, y_val_rgb = all_data['rgb']['validation']\n",
    "        X_test_rgb, y_test_rgb = all_data['rgb']['test']\n",
    "        print(f\"RGB - Training: {X_train_rgb.shape}, Validation: {X_val_rgb.shape}, Test: {X_test_rgb.shape}\")\n",
    "        print(\"Data loading completed successfully!\\n\")\n",
    "        \n",
    "        def compare_stopping_criteria(\n",
    "            X_train, y_train, X_val, y_val, X_test, y_test, data_name=\"Data\",\n",
    "            fixed_epochs=100, error_threshold=0.05, patience=10, min_delta=1e-4, total_epochs=200\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Compare different stopping criteria for training the perceptron.\n",
    "            \"\"\"\n",
    "            criteria_results = {}\n",
    "            \n",
    "            criteria_settings = [\n",
    "                ('fixed_epochs', {'total_epochs': fixed_epochs}),\n",
    "                ('error_threshold', {'total_epochs': total_epochs, 'error_threshold': error_threshold}),\n",
    "                ('early_stopping', {'total_epochs': total_epochs, 'patience': patience, 'min_delta': min_delta})\n",
    "            ]\n",
    "            \n",
    "            for criterion_name, params in criteria_settings:\n",
    "                perceptron = MultiClassPerceptron(\n",
    "                    num_of_features=X_train.shape[1],\n",
    "                    alpha=0.001  # Learning rate (small for stability)\n",
    "                )\n",
    "                trainer = PerceptronTrainer(\n",
    "                    perceptron, X_train, y_train, X_val, y_val,\n",
    "                    stopping_criterion=criterion_name, **params\n",
    "                )\n",
    "                trained_perceptron = trainer.train()\n",
    "                test_accuracy = trainer.accuracy(X_test, y_test)\n",
    "                \n",
    "                criteria_results[criterion_name] = {\n",
    "                    'epochs_trained': trainer.epochs_trained,\n",
    "                    'final_train_acc': trainer.accuracy(X_train, y_train),\n",
    "                    'final_val_acc': trainer.accuracy(X_val, y_val) if X_val is not None else None,\n",
    "                    'test_accuracy': test_accuracy,\n",
    "                    'stopped_reason': trainer.stopped_reason\n",
    "                }\n",
    "                \n",
    "            print(f\"\\nSTOPPING CRITERIA COMPARISON ({data_name})\")\n",
    "            print(\"=\"*60)\n",
    "            for criterion, results in criteria_results.items():\n",
    "                print(f\"{criterion:15s}: {results['epochs_trained']:3d} epochs\")\n",
    "                print(f\"{'':17s} Train: {results['final_train_acc']:.4f}\")\n",
    "                print(f\"{'':17s} Val:   {results['final_val_acc']:.4f}\")\n",
    "                print(f\"{'':17s} Test:  {results['test_accuracy']:.4f}\")\n",
    "                print(f\"{'':17s} Reason: {results['stopped_reason']}\")\n",
    "                print(\"-\"*60)\n",
    "        \n",
    "        compare_stopping_criteria(\n",
    "            X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "            data_name=\"Grayscale\",\n",
    "            fixed_epochs=400, error_threshold=0.25, patience=50, min_delta=0.001, total_epochs=400\n",
    "        )\n",
    "          \n",
    "        compare_stopping_criteria(\n",
    "            X_train_rgb, y_train_rgb, X_val_rgb, y_val_rgb, X_test_rgb, y_test_rgb,\n",
    "            data_name=\"RGB\",\n",
    "            fixed_epochs=600, error_threshold=0.20, patience=25, min_delta=0.001, total_epochs=600\n",
    "        )\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Could not find directory. Please ensure the dataset path is correct.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or training data: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6113b35-9f12-44e9-8076-2a6e8fa78980",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "# 4.Hyperparameter tuning \n",
    "Hyperparameter tuning means trying out different settings to find the best ones for training a machine learning model.  \n",
    "These settings, called **hyperparameters**, control how the model learns but are not learned from the data itself.  \n",
    "They include things like:\n",
    "\n",
    "- **Learning rate** – how fast the model updates during training.  \n",
    "- **Weight initialization** – how the model starts its learning.  \n",
    "- **Input normalization** – how the input data is scaled or adjusted.  \n",
    "\n",
    "When training two models—one using RGB images and the other using grayscale images ,we try different combinations of these hyperparameters to see which settings work best for each input type.\n",
    "\n",
    "### What Each Hyperparameter Does\n",
    "\n",
    "**1. Learning Rate**  \n",
    "Controls how big the steps are when the model updates during training.  \n",
    "- If it’s too high, the model might overshoot and miss the best solution.  \n",
    "- If it’s too low, training can be very slow and take longer to converge.  \n",
    "\n",
    "**2. Weight Initialization**  \n",
    "Determines how the model’s weights are set before training begins.  \n",
    "Common methods include:  \n",
    "- **Zero Initialization:** All weights start at zero.  \n",
    "- **Constant Initialization:** All weights start at the same chosen value.  \n",
    "- **Uniform Initialization:** Weights are randomly selected from a uniform range.  \n",
    "- **Gaussian Initialization:** Weights are randomly selected from a normal (bell curve) distribution.  \n",
    "\n",
    "**3. Normalization Technique**  \n",
    "Adjusts the input data before training so the model learns more effectively.  \n",
    "Options include:  \n",
    "- **No Normalization:** Use raw pixel values directly.  \n",
    "- **Normalization to [0, 1]:** Scale pixel values so they lie between 0 and 1.  \n",
    "- **Z-Score Normalization:** Adjust pixel values based on the mean and standard deviation of the dataset.  \n",
    "\n",
    "By testing different combinations of these hyperparameters, we can find the settings that help each model learn faster and achieve better accuracy.\n",
    "\n",
    "We will now define the `EnhancedBinaryPerceptron` class, which extends the basic `BinaryPerceptron`. This enhanced version supports multiple strategies for initializing the weights, making it ideal for hyperparameter tuning where different starting points can impact training performance.\n",
    "\n",
    "The **__init__** method takes the number of features, a learning rate `alpha`, and an `init_strategy` parameter that can be `'zero'`, `'constant'`, `'uniform'`, or `'gaussian'`. Based on this strategy, the weights are initialized as all zeros, a fixed constant value, random values from a uniform range, or random values from a Gaussian distribution. The bias is always initialized to `0.0`.\n",
    "\n",
    "The **__repr__** method provides a clear summary of the perceptron’s initialization choice, learning rate, and weights. Using this enhanced class makes it possible to experiment with different initialization strategies, which can influence how quickly the model converges and the final accuracy achieved.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a70f05-2581-4ef8-adac-ecae8bf41bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedBinaryPerceptron(BinaryPerceptron):\n",
    "    \"\"\"Enhanced Binary Perceptron with multiple initialization strategies.\"\"\"\n",
    "\n",
    "    def __init__(self, num_of_features, alpha=0.01, init_strategy='gaussian'):\n",
    "        super().__init__(num_of_features, alpha)  # Call the parent constructor\n",
    "        \n",
    "        self.init_strategy = init_strategy\n",
    "\n",
    "        if init_strategy == 'zero':\n",
    "            self.weights = np.zeros(num_of_features)\n",
    "        elif init_strategy == 'constant':\n",
    "            self.weights = np.full(num_of_features, 0.1)\n",
    "        elif init_strategy == 'uniform':\n",
    "            self.weights = np.random.uniform(-0.1, 0.1, num_of_features)\n",
    "        elif init_strategy == 'gaussian':\n",
    "            self.weights = np.random.normal(0, 0.1, num_of_features)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown initialization strategy: {init_strategy}\")\n",
    "\n",
    "        self.bias = 0.0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"EnhancedBinaryPerceptron(init={self.init_strategy}, alpha={self.alpha})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b173dffb-e092-40fc-b038-e298ae85e639",
   "metadata": {},
   "source": [
    "`EnhancedMultiClassPerceptron` class which extends the multi-class perceptron by using `EnhancedBinaryPerceptron` instances.  \n",
    "This class follows a **One-vs-Rest** approach, creating one enhanced binary perceptron for each class in the dataset.\n",
    "\n",
    "The **__init__** method takes the number of features, a learning rate `alpha`, an `init_strategy` for weight initialization, and the number of classes.  \n",
    "For reproducibility, a different random seed is used for each perceptron, and each one is initialized according to the chosen strategy.  \n",
    "This design allows experimentation with different weight starting points across all classes.\n",
    "\n",
    "The **predict** method computes the scores from all perceptrons and selects the class with the highest score for each sample.  \n",
    "\n",
    "The **apply_learning_rule** method updates all perceptrons using a One-vs-Rest approach:  \n",
    "the perceptron corresponding to the correct class is updated with a positive target (`1`), while all others are updated with a negative target (`0`).  \n",
    "\n",
    "Finally, the **__repr__** method provides a concise summary of the model’s initialization strategy, learning rate, and number of classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed4fda-a672-4c17-a3e2-98fc162b5c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedMultiClassPerceptron:\n",
    "    \"\"\"Enhanced Multi-class Perceptron using EnhancedBinaryPerceptrons.\"\"\"\n",
    "\n",
    "    def __init__(self, num_of_features, alpha=0.01, init_strategy='gaussian', num_classes=10):\n",
    "        self.init_strategy = init_strategy\n",
    "        self.perceptrons = []\n",
    "\n",
    "       \n",
    "        for i in range(num_classes):  # Create one EnhancedBinaryPerceptron per class\n",
    "            np.random.seed(42 + i)  # reproducible but varied\n",
    "            perceptron = EnhancedBinaryPerceptron(num_of_features, alpha, init_strategy)\n",
    "            self.perceptrons.append(perceptron)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels for samples in X.\"\"\"\n",
    "        scores = np.array([p.weighted_sum(X) for p in self.perceptrons]).T\n",
    "        return np.argmax(scores, axis=1)\n",
    "\n",
    "    def apply_learning_rule(self, x, y_true):\n",
    "        \"\"\"Update all perceptrons (1-vs-rest scheme).\"\"\"\n",
    "        for i, perceptron in enumerate(self.perceptrons):\n",
    "            target = 1 if y_true == i else 0\n",
    "            perceptron.apply_learning_rule(x, target)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"EnhancedMultiClassPerceptron(init={self.init_strategy}, alpha={self.perceptrons[0].alpha})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0137832-d123-4a5e-b11d-b23a8009a2a3",
   "metadata": {},
   "source": [
    "The `EnhancedPerceptronTrainer` class, which is designed to train a perceptron model with different stopping criteria and performance monitoring.  \n",
    "This trainer can work with both training and validation datasets, and supports three stopping conditions and fixed number of epochs, reaching a target error threshold, or early stopping with a patience parameter based on validation accuracy.\n",
    "\n",
    "The **shuffle_data** method randomizes the order of samples at the start of each epoch, helping the model generalize better by reducing bias from sample order.  \n",
    "The **accuracy** method computes the fraction of correctly predicted labels for any dataset. The **stopping_criteria** method checks whether training should stop, monitoring validation accuracy when early stopping is selected.\n",
    "\n",
    "The **train** method runs the full training loop and it shuffles the data, applies the perceptron learning rule to each sample, and tracks training and validation accuracy after every epoch.  \n",
    "Validation results are stored to detect improvements over time, and training halts once the chosen stopping condition is satisfied.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc1efb-6e9c-4c3f-8706-71aed96f8bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    " class EnhancedPerceptronTrainer:\n",
    "    def __init__(self, perceptron, X_train, y_train, X_val=None, y_val=None,\n",
    "                 stopping_criterion=\"fixed_epochs\", total_epochs=50,\n",
    "                 error_threshold=0.05, patience=5, min_delta=1e-4):\n",
    "        self.perceptron = perceptron\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_val, self.y_val = X_val, y_val\n",
    "        self.stopping_criterion, self.total_epochs = stopping_criterion, total_epochs\n",
    "        self.error_threshold, self.patience, self.min_delta = error_threshold, patience, min_delta\n",
    "\n",
    "        self.epochs_trained = 0\n",
    "        self.stopped_reason = None\n",
    "        self.epochs_without_improvement = 0\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def shuffle_data(self, X, y):\n",
    "        idx = np.random.permutation(len(X))\n",
    "        return X[idx], y[idx]\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        return np.mean(self.perceptron.predict(X) == y)\n",
    "\n",
    "    def stopping_criteria(self, epoch, train_error, val_acc=None):\n",
    "        if self.stopping_criterion == 'fixed_epochs':\n",
    "            return epoch >= self.total_epochs\n",
    "\n",
    "        elif self.stopping_criterion == 'error_threshold':\n",
    "            return (train_error <= self.error_threshold) or (epoch >= self.total_epochs)\n",
    "\n",
    "        elif self.stopping_criterion == 'early_stopping':\n",
    "            if val_acc is None:\n",
    "                raise ValueError(\"Validation data required for early stopping\")\n",
    "            best_val = max(self.val_accuracies) if self.val_accuracies else 0\n",
    "            if val_acc <= best_val + self.min_delta:\n",
    "                self.epochs_without_improvement += 1\n",
    "            else:\n",
    "                self.epochs_without_improvement = 0\n",
    "            return self.epochs_without_improvement >= self.patience or epoch >= self.total_epochs\n",
    "\n",
    "        return False\n",
    "\n",
    "    def train(self, verbose=False):\n",
    "        for epoch in range(1, self.total_epochs + 1):\n",
    "            X_shuf, y_shuf = self.shuffle_data(self.X_train, self.y_train)\n",
    "\n",
    "            for x, y in zip(X_shuf, y_shuf):\n",
    "                self.perceptron.apply_learning_rule(x, y)\n",
    "\n",
    "            train_acc = self.accuracy(self.X_train, self.y_train)\n",
    "            train_error = 1 - train_acc\n",
    "            val_acc = self.accuracy(self.X_val, self.y_val) if self.X_val is not None else None\n",
    "\n",
    "            if val_acc is not None:\n",
    "                self.val_accuracies.append(val_acc)\n",
    "\n",
    "            self.epochs_trained = epoch\n",
    "\n",
    "            if self.stopping_criteria(epoch, train_error, val_acc):\n",
    "                if verbose:\n",
    "                    print(f\"Stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch}: Train={train_acc:.4f}, Val={val_acc:.4f}\" if val_acc else f\"Epoch {epoch}: Train={train_acc:.4f}\")\n",
    "\n",
    "        return self.perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eeaa9a-81ac-45cf-a3ad-1307218fe023",
   "metadata": {},
   "source": [
    "We will now define the `DataNormalizer`class, which is a helper class used to preprocess input data before passing it to a perceptron model.  \n",
    "This class provides a static **normalize** method that supports multiple normalization strategies.\n",
    "\n",
    "If `strategy='none'`, the data is returned unchanged.  \n",
    "If `strategy='minmax'`, pixel values are scaled to the range `[0, 1]` by dividing by `255.0`.  \n",
    "If `strategy='zscore'`, the data is standardized by subtracting the mean and dividing by the standard deviation for each feature, with safe handling for features with zero variance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a640d9a4-7276-4ef3-bc45-6fd4a911c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataNormalizer:\n",
    "    def normalize(X, strategy='minmax'):\n",
    "        if strategy == 'none': return X\n",
    "        elif strategy == 'minmax': return X.astype(np.float32) / 255.0\n",
    "        elif strategy == 'zscore':\n",
    "            mean = np.mean(X, axis=0, keepdims=True)\n",
    "            std = np.std(X, axis=0, keepdims=True)\n",
    "            std[std == 0] = 1\n",
    "            return (X - mean) / std\n",
    "        else: raise ValueError(f\"Unknown normalization strategy: {strategy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3432b05d-ffbb-47de-91fa-9f2a062b8aa0",
   "metadata": {},
   "source": [
    "We will now define the `HyperparameterTuner` class, which automates the search for the best training settings for `EnhancedMultiClassPerceptron` models on both RGB and grayscale datasets.\n",
    "\n",
    "The class takes training, validation, and test datasets, along with a mode (`\"rgb\"` or `\"grayscale\"`) that controls which hyperparameter ranges to explore. The hyperparameters include the learning rate, weight initialization strategy, and input normalization method. For RGB data, early stopping with patience is applied, while grayscale data uses a fixed number of epochs.\n",
    "\n",
    "The **run_search** method performs a grid search over all combinations of hyperparameters. For each configuration, it normalizes the data with `DataNormalizer`, initializes the perceptron with the chosen weight strategy, trains it with `EnhancedPerceptronTrainer`, and evaluates performance on the training, validation, and test sets. It records key metrics such as accuracy, number of epochs trained, training time, and stopping reason.\n",
    "\n",
    "Once all combinations are tested, the results are sorted by test accuracy, and the top five configurations are printed.  \n",
    "Finally, the best-performing setup is highlighted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fc661e-8599-47aa-80c5-268fada55717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class HyperparameterTuner:\n",
    "    \"\"\"Hyperparameter tuner for RGB and Grayscale MultiClassPerceptrons using DataNormalizer.\"\"\"\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val, X_test, y_test, mode=\"rgb\"):\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_val, self.y_val = X_val, y_val\n",
    "        self.X_test, self.y_test = X_test, y_test\n",
    "        self.mode = mode.lower()\n",
    "        \n",
    "        if self.mode == \"rgb\":\n",
    "            self.learning_rates = [0.01, 0.005, 0.001, 0.0005]\n",
    "            self.init_strategies = ['gaussian', 'uniform', 'constant', 'zero']\n",
    "            self.norm_strategies = ['minmax', 'zscore', 'none']\n",
    "            self.total_epochs = 50\n",
    "            self.stopping_criterion = \"early_stopping\"\n",
    "            self.patience = 10\n",
    "            self.min_delta = 0.001\n",
    "        elif self.mode == \"grayscale\":\n",
    "            self.learning_rates = [0.1, 0.05, 0.01, 0.005]\n",
    "            self.init_strategies = ['gaussian', 'uniform', 'constant']\n",
    "            self.norm_strategies = ['minmax', 'zscore']\n",
    "            self.total_epochs = 30\n",
    "            self.stopping_criterion = \"fixed_epochs\"\n",
    "            self.patience = None\n",
    "            self.min_delta = None\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'rgb' or 'grayscale'\")\n",
    "\n",
    "        self.all_results = []\n",
    "        self.best_config = {'test_acc': 0, 'config': None, 'model': None}\n",
    "\n",
    "    def run_search(self):\n",
    "        \"\"\"Run grid search over learning rates, initialization, and normalization.\"\"\"\n",
    "        total_combinations = len(self.learning_rates) * len(self.init_strategies) * len(self.norm_strategies)\n",
    "        combo_count = 0\n",
    "        print(f\"HYPERPARAMETER SEARCH - {self.mode.upper()}\")\n",
    "\n",
    "        for lr in self.learning_rates:\n",
    "            for init_strat in self.init_strategies:\n",
    "                for norm_strat in self.norm_strategies:\n",
    "                    combo_count += 1\n",
    "                    print(f\"\\n--- Combo {combo_count}/{total_combinations}: \"\n",
    "                          f\"lr={lr}, init={init_strat}, norm={norm_strat} ---\")\n",
    "                    try:\n",
    "                        # Normalize using DataNormalizer\n",
    "                        X_train_norm = DataNormalizer.normalize(self.X_train.copy(), norm_strat)\n",
    "                        X_val_norm = DataNormalizer.normalize(self.X_val.copy(), norm_strat)\n",
    "                        X_test_norm = DataNormalizer.normalize(self.X_test.copy(), norm_strat)\n",
    "\n",
    "                        # Initialize model\n",
    "                        perceptron = EnhancedMultiClassPerceptron(\n",
    "                            num_of_features=X_train_norm.shape[1],\n",
    "                            alpha=lr,\n",
    "                            init_strategy=init_strat\n",
    "                        )\n",
    "\n",
    "                        # Setup trainer arguments\n",
    "                        trainer_args = {\n",
    "                            'perceptron': perceptron,\n",
    "                            'X_train': X_train_norm,\n",
    "                            'y_train': self.y_train,\n",
    "                            'X_val': X_val_norm,\n",
    "                            'y_val': self.y_val,\n",
    "                            'stopping_criterion': self.stopping_criterion,\n",
    "                            'total_epochs': self.total_epochs\n",
    "                        }\n",
    "                        if self.stopping_criterion == \"early_stopping\":\n",
    "                            trainer_args['patience'] = self.patience\n",
    "                            trainer_args['min_delta'] = self.min_delta\n",
    "\n",
    "                        trainer = EnhancedPerceptronTrainer(**trainer_args)\n",
    "\n",
    "                        # Train and measure time\n",
    "                        start_time = time.time()\n",
    "                        trained_model = trainer.train(verbose=False)\n",
    "                        training_time = time.time() - start_time\n",
    "\n",
    "                        # Evaluate\n",
    "                        test_acc = trainer.accuracy(X_test_norm, self.y_test)\n",
    "                        val_acc = trainer.accuracy(X_val_norm, self.y_val)\n",
    "                        train_acc = trainer.accuracy(X_train_norm, self.y_train)\n",
    "\n",
    "                        # Record results\n",
    "                        result = {\n",
    "                            'learning_rate': lr,\n",
    "                            'init_strategy': init_strat,\n",
    "                            'norm_strategy': norm_strat,\n",
    "                            'epochs_trained': trainer.epochs_trained,\n",
    "                            'train_acc': train_acc,\n",
    "                            'val_acc': val_acc,\n",
    "                            'test_acc': test_acc,\n",
    "                            'training_time': training_time,\n",
    "                            'stopped_reason': trainer.stopped_reason,\n",
    "                            'X_val_norm': X_val_norm  # store for evaluation\n",
    "                        }\n",
    "                        self.all_results.append(result)\n",
    "\n",
    "                        # Update best model\n",
    "                        if test_acc > self.best_config['test_acc']:\n",
    "                            self.best_config['test_acc'] = test_acc\n",
    "                            self.best_config['config'] = result\n",
    "                            self.best_config['model'] = trained_model\n",
    "\n",
    "                        print(f\"  → Test Acc: {test_acc:.4f}, Val Acc: {val_acc:.4f}, \"\n",
    "                              f\"Train Acc: {train_acc:.4f}, Epochs: {trainer.epochs_trained}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"  → ERROR: {str(e)}\")\n",
    "                        continue\n",
    "        self.all_results.sort(key=lambda x: x['test_acc'], reverse=True)\n",
    "        print(f\"\\nTOP 5 CONFIGURATIONS:\")\n",
    "        for i, res in enumerate(self.all_results[:5]):\n",
    "            print(f\"{i+1}. Test Acc: {res['test_acc']:.4f} | LR: {res['learning_rate']} | \"\n",
    "                  f\"Init: {res['init_strategy']:8s} | Norm: {res['norm_strategy']:6s} | \"\n",
    "                  f\"Epochs: {res['epochs_trained']:3d}\")\n",
    "\n",
    "        best = self.best_config['config']\n",
    "        print(f\"\\nBEST CONFIGURATION:\")\n",
    "        print(f\"Learning Rate: {best['learning_rate']}\")\n",
    "        print(f\"Initialization: {best['init_strategy']}\")\n",
    "        print(f\"Normalization: {best['norm_strategy']}\")\n",
    "        print(f\"Test Accuracy: {best['test_acc']:.4f}\")\n",
    "        print(f\"Validation Accuracy: {best['val_acc']:.4f}\")\n",
    "        print(f\"Training Accuracy: {best['train_acc']:.4f}\")\n",
    "        print(f\"Epochs Trained: {best['epochs_trained']}\")\n",
    "        print(f\"Training Time: {best['training_time']:.2f}s\")\n",
    "\n",
    "        return self.all_results, self.best_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93962853-54b4-44fc-babf-224f1cb4b7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    loader = Loader('simpsons-mnist-master/dataset')  # Initialize the loader class.\n",
    "    try:\n",
    "        print(\"Data preprocessing in progress......\")\n",
    "        all_data = loader.load_all_data()\n",
    "        \n",
    "        if all_data is None:\n",
    "            raise ValueError(\"Data loader returned None\")\n",
    "\n",
    "        X_train, y_train = all_data['grayscale']['train']\n",
    "        X_val, y_val = all_data['grayscale']['validation']\n",
    "        X_test, y_test = all_data['grayscale']['test']\n",
    "\n",
    "        X_train_rgb, y_train_rgb = all_data['rgb']['train']\n",
    "        X_val_rgb, y_val_rgb = all_data['rgb']['validation']\n",
    "        X_test_rgb, y_test_rgb = all_data['rgb']['test']\n",
    "\n",
    "        print(f\"Grayscale - Training: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        print(f\"RGB - Training: {X_train_rgb.shape}, Validation: {X_val_rgb.shape}, Test: {X_test_rgb.shape}\")\n",
    "        print(\"Data loading completed successfully!\\n\")\n",
    " \n",
    "        rgb_tuner = HyperparameterTuner(X_train_rgb, y_train_rgb, X_val_rgb, y_val_rgb, X_test_rgb, y_test_rgb, mode=\"rgb\")\n",
    "        rgb_results, rgb_best_config = rgb_tuner.run_search()\n",
    "        \n",
    "        grayscale_tuner = HyperparameterTuner(X_train, y_train, X_val, y_val, X_test, y_test, mode=\"grayscale\")\n",
    "        grayscale_results, grayscale_best_config = grayscale_tuner.run_search()\n",
    "        \n",
    "        print(\"COMPARATIVE ANALYSIS\")\n",
    "        rgb_best = max(rgb_results, key=lambda x: x['test_acc'])\n",
    "        grayscale_best = max(grayscale_results, key=lambda x: x['test_acc'])\n",
    "\n",
    "        print(f\"\\nBEST RGB PERFORMANCE:\")\n",
    "        print(f\"  Test Accuracy: {rgb_best['test_acc']:.4f}\")\n",
    "        print(f\"  Configuration: LR={rgb_best['learning_rate']}, Init={rgb_best['init_strategy']}, Norm={rgb_best['norm_strategy']}\")\n",
    "\n",
    "        print(f\"\\nBEST GRAYSCALE PERFORMANCE:\")\n",
    "        print(f\"  Test Accuracy: {grayscale_best['test_acc']:.4f}\")\n",
    "        print(f\"  Configuration: LR={grayscale_best['learning_rate']}, Init={grayscale_best['init_strategy']}, Norm={grayscale_best['norm_strategy']}\")\n",
    "\n",
    "        print(f\"\\nPERFORMANCE GAP: {rgb_best['test_acc'] - grayscale_best['test_acc']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or training data: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f62582-a514-4d35-a392-fff8c0aeeb04",
   "metadata": {},
   "source": [
    "### Imports, Installations, and Downloads\n",
    "\n",
    "We will use standard Python libraries for evaluation:\n",
    "\n",
    "- **numpy** – for storing and manipulating arrays and matrices of numerical data.  \n",
    "- **matplotlib** – for visualizing plots, including image data and training results.  \n",
    "- **sklearn.metrics** – for evaluating model performance, including accuracy, precision, recall, F1-score, and confusion matrix visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22ae67-4227-4b6d-b078-7a776cb5a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac175f7-1418-4bf5-b20c-c203130d7567",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "# 5. Evaluation\n",
    "We will now define the `Evaluator` class, which is responsible for assessing a trained model’s performance on validation and test data.  \n",
    "This class provides a single **evaluate** method that computes several key metrics and visualizes results.\n",
    "\n",
    "The method first predicts labels for the validation set and calculates validation accuracy.  It then predicts on the test set, computing test accuracy, precision, recall, and F1-score for each class. These metrics give a detailed view of the model’s strengths and weaknesses across different classes.\n",
    "\n",
    "A confusion matrix is generated using `sklearn.metrics.confusion_matrix`, and visualized with `ConfusionMatrixDisplay`, showing how often each class is correctly or incorrectly predicted.  The confusion matrix is displayed as a heatmap with class names on both axes, making errors easy to interpret.\n",
    "\n",
    "Finally, the method returns a dictionary containing validation accuracy, test accuracy, and per-class precision, recall, and F1-score, which can be logged or compared across models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed551f7-39f7-49e2-8ecb-77d9a00ea3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def evaluate(model, X_val, y_val, X_test, y_test, class_names, title=\"Model Evaluation\"):\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_acc = accuracy_score(y_val, y_val_pred)\n",
    "        \n",
    "        y_test_pred = model.predict(X_test)\n",
    "        test_acc = accuracy_score(y_test, y_test_pred)\n",
    "        precision = precision_score(y_test, y_test_pred, average=None, zero_division=0)\n",
    "        recall = recall_score(y_test, y_test_pred, average=None, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_test_pred, average=None, zero_division=0)\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_test_pred, labels=range(len(class_names)))\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "        disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
    "        plt.title(f\"{title} - Confusion Matrix\")\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"test_accuracy\": test_acc,\n",
    "            \"precision_per_class\": precision.tolist(),\n",
    "            \"recall_per_class\": recall.tolist(),\n",
    "            \"f1_per_class\": f1.tolist()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2bf183-a72b-4372-a850-2bb321dbdf28",
   "metadata": {},
   "outputs": [],
   "source": [
    " if __name__ == \"__main__\":\n",
    "    try:\n",
    "        loader = Loader('simpsons-mnist-master/dataset')\n",
    "        all_data = loader.load_all_data()\n",
    "        \n",
    "        X_train_g, y_train_g = all_data['grayscale']['train']\n",
    "        X_val_g, y_val_g = all_data['grayscale']['validation']\n",
    "        X_test_g, y_test_g = all_data['grayscale']['test']\n",
    "        \n",
    "        X_train_rgb, y_train_rgb = all_data['rgb']['train']\n",
    "        X_val_rgb, y_val_rgb = all_data['rgb']['validation']\n",
    "        X_test_rgb, y_test_rgb = all_data['rgb']['test']\n",
    "        \n",
    "        class_names = [str(i) for i in range(10)]\n",
    "        lr_list = [0.01, 0.005, 0.001]\n",
    "        init_list = ['gaussian', 'uniform']\n",
    "        norm_list = ['minmax', 'zscore']\n",
    "        print(\"\\nTuning hyperparameters for Grayscale dataset...\")\n",
    "        grayscale_tuner = HyperparameterTuner(\n",
    "            X_train_g, y_train_g, X_val_g, y_val_g, X_test_g, y_test_g, mode=\"grayscale\"\n",
    "        )\n",
    "        grayscale_results, grayscale_best_config = grayscale_tuner.run_search()\n",
    "        print(\"\\nTuning hyperparameters for RGB dataset...\")\n",
    "        rgb_tuner = HyperparameterTuner(\n",
    "            X_train_rgb, y_train_rgb, X_val_rgb, y_val_rgb, X_test_rgb, y_test_rgb, mode=\"rgb\"\n",
    "        )\n",
    "        rgb_results, rgb_best_config = rgb_tuner.run_search()\n",
    "        print(\"\\nEvaluating Grayscale best model...\")\n",
    "        gray_metrics = Evaluator.evaluate(\n",
    "            model=grayscale_best_config['model'],\n",
    "            X_val=grayscale_best_config['config']['X_val_norm'],\n",
    "            y_val=y_val_g,\n",
    "            X_test=DataNormalizer.normalize(X_test_g.copy(), grayscale_best_config['config']['norm_strategy']),\n",
    "            y_test=y_test_g,\n",
    "            class_names=class_names,\n",
    "            title=\"Grayscale Final Model\"\n",
    "        )\n",
    "        print(\"\\nEvaluating RGB best model...\")\n",
    "        rgb_metrics = Evaluator.evaluate(\n",
    "            model=rgb_best_config['model'],\n",
    "            X_val=rgb_best_config['config']['X_val_norm'],\n",
    "            y_val=y_val_rgb,\n",
    "            X_test=DataNormalizer.normalize(X_test_rgb.copy(), rgb_best_config['config']['norm_strategy']),\n",
    "            y_test=y_test_rgb,\n",
    "            class_names=class_names,\n",
    "            title=\"RGB Final Model\"\n",
    "        )\n",
    "        print(\"\\nComparison of final models:\")\n",
    "        print(\"\\nGrayscale metrics:\", gray_metrics)\n",
    "        print(\"\\nRGB metrics:\", rgb_metrics)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during loading, training or evaluation:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee081b-0e0e-4b13-bbc0-2263572681d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as n\n",
    "class PredictionVisualizer:\n",
    "    def __init__(self, model, class_names=None, is_rgb=False):\n",
    "        \"\"\"\n",
    "        Initialize a visualizer for predictions.\n",
    "\n",
    "        model: trained perceptron (binary or multi-class)\n",
    "        class_names: optional list of labels for mapping numeric outputs to names\n",
    "        is_rgb: True if dataset is RGB, False if grayscale\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.class_names = class_names\n",
    "        self.is_rgb = is_rgb\n",
    "\n",
    "    def _predict_label(self, x):\n",
    "        \"\"\"Predicts the class label for a single input x.\"\"\"\n",
    "        output = self.model.predict(x.reshape(1, -1))  # use predict instead of forward\n",
    "        pred_label = output[0] if hasattr(output, \"__len__\") else output\n",
    "        return pred_label\n",
    "\n",
    "    def show_correct_only(self, X, y, n_samples=9):\n",
    "        \"\"\"\n",
    "        Display a grid of n correctly predicted samples.\n",
    "\n",
    "        X: input data\n",
    "        y: true labels\n",
    "        n_samples: number of correct predictions to display\n",
    "        \"\"\"\n",
    "        # Predict all labels\n",
    "        y_pred = self.model.predict(X)\n",
    "\n",
    "        # Find indices where predictions are correct\n",
    "        correct_indices = np.where(y_pred == y)[0]\n",
    "\n",
    "        if len(correct_indices) == 0:\n",
    "            print(\"No correctly predicted samples found.\")\n",
    "            return\n",
    "\n",
    "        # Randomly choose up to n_samples\n",
    "        chosen = np.random.choice(correct_indices, size=min(n_samples, len(correct_indices)), replace=False)\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        for i, idx in enumerate(chosen):\n",
    "            x_sample = X[idx]\n",
    "            true_label = y[idx]\n",
    "            pred_label = y_pred[idx]\n",
    "\n",
    "            true_name = self.class_names[true_label] if self.class_names else str(true_label)\n",
    "            pred_name = self.class_names[pred_label] if self.class_names else str(pred_label)\n",
    "\n",
    "            plt.subplot(int(np.sqrt(n_samples)), int(np.sqrt(n_samples)), i + 1)\n",
    "            if self.is_rgb:\n",
    "                img = x_sample.reshape(28, 28, 3)\n",
    "                plt.imshow(img.astype(np.uint8))\n",
    "            else:\n",
    "                img = x_sample.reshape(28, 28)\n",
    "                plt.imshow(img, cmap=\"gray\")\n",
    "\n",
    "            plt.title(f\"T:{true_name}\\nP:{pred_name}\", fontsize=8)\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d15764-c632-460a-ba6a-275170d1d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        loader = Loader('simpsons-mnist-master/dataset')\n",
    "        all_data = loader.load_all_data()\n",
    "\n",
    "        X_train_g, y_train_g = all_data['grayscale']['train']\n",
    "        X_val_g, y_val_g = all_data['grayscale']['validation']\n",
    "        X_test_g, y_test_g = all_data['grayscale']['test']\n",
    "\n",
    "        X_train_rgb, y_train_rgb = all_data['rgb']['train']\n",
    "        X_val_rgb, y_val_rgb = all_data['rgb']['validation']\n",
    "        X_test_rgb, y_test_rgb = all_data['rgb']['test']\n",
    "\n",
    "        class_names = [str(i) for i in range(10)]\n",
    "\n",
    "        lr_list = [0.01, 0.005, 0.001]\n",
    "        init_list = ['gaussian', 'uniform']\n",
    "        norm_list = ['minmax', 'zscore']\n",
    " \n",
    "        print(\"\\nTuning hyperparameters for Grayscale dataset...\")\n",
    "        grayscale_tuner = HyperparameterTuner(\n",
    "            X_train_g, y_train_g, X_val_g, y_val_g, X_test_g, y_test_g, mode=\"grayscale\"\n",
    "        )\n",
    "        grayscale_results, grayscale_best_config = grayscale_tuner.run_search()\n",
    " \n",
    "        print(\"\\nTuning hyperparameters for RGB dataset...\")\n",
    "        rgb_tuner = HyperparameterTuner(\n",
    "            X_train_rgb, y_train_rgb, X_val_rgb, y_val_rgb, X_test_rgb, y_test_rgb, mode=\"rgb\"\n",
    "        )\n",
    "        rgb_results, rgb_best_config = rgb_tuner.run_search()\n",
    "        \n",
    "        print(\"\\nEvaluating Grayscale best model...\")\n",
    "        gray_metrics = Evaluator.evaluate(\n",
    "            model=grayscale_best_config['model'],\n",
    "            X_val=grayscale_best_config['config']['X_val_norm'],\n",
    "            y_val=y_val_g,\n",
    "            X_test=DataNormalizer.normalize(\n",
    "                X_test_g.copy(), grayscale_best_config['config']['norm_strategy']\n",
    "            ),\n",
    "            y_test=y_test_g,\n",
    "            class_names=class_names,\n",
    "            title=\"Grayscale Final Model\"\n",
    "        )\n",
    "\n",
    "        print(\"\\nEvaluating RGB best model...\")\n",
    "        rgb_metrics = Evaluator.evaluate(\n",
    "            model=rgb_best_config['model'],\n",
    "            X_val=rgb_best_config['config']['X_val_norm'],\n",
    "            y_val=y_val_rgb,\n",
    "            X_test=DataNormalizer.normalize(\n",
    "                X_test_rgb.copy(), rgb_best_config['config']['norm_strategy']\n",
    "            ),\n",
    "            y_test=y_test_rgb,\n",
    "            class_names=class_names,\n",
    "            title=\"RGB Final Model\"\n",
    "        )\n",
    "\n",
    "        print(\"\\nComparison of final models:\")\n",
    "        print(\"\\nGrayscale metrics:\", gray_metrics)\n",
    "        print(\"\\nRGB metrics:\", rgb_metrics)\n",
    "        print(\"\\nVisualizing 9 correctly predicted samples...\")\n",
    "\n",
    "        gray_vis = PredictionVisualizer(\n",
    "            model=grayscale_best_config['model'],\n",
    "            class_names=class_names,\n",
    "            is_rgb=False\n",
    "        )\n",
    "        gray_vis.show_correct_only(X_val_g, y_val_g, n_samples=9)\n",
    "\n",
    "        rgb_vis = PredictionVisualizer(\n",
    "            model=rgb_best_config['model'],\n",
    "            class_names=class_names,\n",
    "            is_rgb=True\n",
    "        )\n",
    "        rgb_vis.show_correct_only(X_val_rgb, y_val_rgb, n_samples=9)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during loading, training or evaluation:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
